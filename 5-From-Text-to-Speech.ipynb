{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNM7a94yA0Wte3Kfr870dKp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TirendazAcademy/An-LLM-App-with-Chainlit/blob/main/5-From-Text-to-Speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "epPkfVTgW33W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-trained models for text-to-speech"
      ],
      "metadata": {
        "id": "OWkF423iXf9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SpeechT5"
      ],
      "metadata": {
        "id": "2HJ609sBXZ7D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMnKgOHLUNnh"
      },
      "outputs": [],
      "source": [
        "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech\n",
        "\n",
        "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(text=\"Don't count the days, make the days count.\", return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "Zv-8DavhWrnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
        "\n",
        "import torch\n",
        "\n",
        "speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)"
      ],
      "metadata": {
        "id": "B2GkUKe9Wuyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spectrogram = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings)"
      ],
      "metadata": {
        "id": "1D9lnHRAXFd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SpeechT5HifiGan\n",
        "\n",
        "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")"
      ],
      "metadata": {
        "id": "sUWLxeeiXGgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)"
      ],
      "metadata": {
        "id": "puVisQOqXLct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "Audio(speech, rate=16000)"
      ],
      "metadata": {
        "id": "NULbps6dXQeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bark"
      ],
      "metadata": {
        "id": "-2z6ZC9qXjq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bark is a transformer-based text-to-speech model proposed by Suno AI in suno-ai/bark."
      ],
      "metadata": {
        "id": "MQu0YFxgXogD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BarkModel, BarkProcessor\n",
        "\n",
        "model = BarkModel.from_pretrained(\"suno/bark-small\")\n",
        "processor = BarkProcessor.from_pretrained(\"suno/bark-small\")"
      ],
      "metadata": {
        "id": "dJPxsxsYXmKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add a speaker embedding\n",
        "inputs = processor(\"This is a test!\", voice_preset=\"v2/en_speaker_3\")\n",
        "\n",
        "speech_output = model.generate(**inputs).cpu().numpy()"
      ],
      "metadata": {
        "id": "rwn3-FblX2dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try it in French, let's also add a French speaker embedding\n",
        "inputs = processor(\"C'est un test!\", voice_preset=\"v2/fr_speaker_1\")\n",
        "\n",
        "speech_output = model.generate(**inputs).cpu().numpy()"
      ],
      "metadata": {
        "id": "-PwD-eiDYENd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try it in French, let's also add a French speaker embedding\n",
        "inputs = processor(\"C'est un test!\", voice_preset=\"v2/fr_speaker_1\")\n",
        "\n",
        "speech_output = model.generate(**inputs).cpu().numpy()"
      ],
      "metadata": {
        "id": "hTWEGpswYEsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(\n",
        "    \"♪ In the mighty jungle, I'm trying to generate barks.\",\n",
        ")\n",
        "\n",
        "speech_output = model.generate(**inputs).cpu().numpy()"
      ],
      "metadata": {
        "id": "w6cX_9c8YMMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_list = [\n",
        "    \"[clears throat] Hello uh ..., my dog is cute [laughter]\",\n",
        "    \"Let's try generating speech, with Bark, a text-to-speech model\",\n",
        "    \"♪ In the jungle, the mighty jungle, the lion barks tonight ♪\",\n",
        "]\n",
        "\n",
        "# also add a speaker embedding\n",
        "inputs = processor(input_list, voice_preset=\"v2/en_speaker_3\")\n",
        "\n",
        "speech_output = model.generate(**inputs).cpu().numpy()"
      ],
      "metadata": {
        "id": "N_jUSh9oYPJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "sampling_rate = model.generation_config.sample_rate\n",
        "Audio(speech_output[0], rate=sampling_rate)"
      ],
      "metadata": {
        "id": "Bn4_C1m5YS3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(speech_output[1], rate=sampling_rate)"
      ],
      "metadata": {
        "id": "Ed5bR_FNY267"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(speech_output[2], rate=sampling_rate)"
      ],
      "metadata": {
        "id": "oLI0EA6SY8Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Massive Multilingual Speech (MMS)"
      ],
      "metadata": {
        "id": "IwyKwWWbYd9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers\n",
        "!pip install --no-cache-dir transformers"
      ],
      "metadata": {
        "id": "CU3akW6HZyHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "DXEZDTo2Z4AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "id": "5p_D9Z1aYfeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import VitsModel, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model = VitsModel.from_pretrained(\"facebook/mms-tts-deu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/mms-tts-deu\")"
      ],
      "metadata": {
        "id": "hQPki7c9ZZwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_example = (\n",
        "    \"Ich bin Schnappi das kleine Krokodil, komm aus Ägypten das liegt direkt am Nil.\"\n",
        ")"
      ],
      "metadata": {
        "id": "-aE0z4yPZGmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "inputs = tokenizer(text_example, return_tensors=\"pt\")\n",
        "input_ids = inputs[\"input_ids\"]\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "\n",
        "speech = outputs[\"waveform\"]"
      ],
      "metadata": {
        "id": "_DZBYDRwZKGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "Audio(speech, rate=16000)"
      ],
      "metadata": {
        "id": "Yf5Zv6ZSaFRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning SpeechT5"
      ],
      "metadata": {
        "id": "B3UiVgEDaJxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## House-keeping"
      ],
      "metadata": {
        "id": "snzTAjgKaSYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "9qVN_2FMaSLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU transformers datasets soundfile speechbrain accelerate"
      ],
      "metadata": {
        "id": "SN-H9GcxaNXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## The dataset"
      ],
      "metadata": {
        "id": "emCMnIHgaul9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Audio\n",
        "\n",
        "dataset = load_dataset(\"facebook/voxpopuli\", \"nl\", split=\"train\", trust_remote_code=True)"
      ],
      "metadata": {
        "id": "aqOKc2h2awMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "id": "e_vZXPNLcrpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ],
      "metadata": {
        "id": "zxxb2CN7a2EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the data"
      ],
      "metadata": {
        "id": "QaOOtAksa7Sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SpeechT5Processor\n",
        "\n",
        "checkpoint = \"microsoft/speecht5_tts\"\n",
        "processor = SpeechT5Processor.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "fB0lmOzia50I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = processor.tokenizer"
      ],
      "metadata": {
        "id": "EmhBRWTrbEgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "wgBrG-GwbE_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_all_chars(batch):\n",
        "    all_text = \" \".join(batch[\"normalized_text\"])\n",
        "    vocab = list(set(all_text))\n",
        "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
        "\n",
        "\n",
        "vocabs = dataset.map(\n",
        "    extract_all_chars,\n",
        "    batched=True,\n",
        "    batch_size=-1,\n",
        "    keep_in_memory=True,\n",
        "    remove_columns=dataset.column_names,\n",
        ")\n",
        "\n",
        "dataset_vocab = set(vocabs[\"vocab\"][0])\n",
        "tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}"
      ],
      "metadata": {
        "id": "mc8gYB2AbGi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_vocab - tokenizer_vocab"
      ],
      "metadata": {
        "id": "Hh08dTIFbPWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replacements = [\n",
        "    (\"à\", \"a\"),\n",
        "    (\"ç\", \"c\"),\n",
        "    (\"è\", \"e\"),\n",
        "    (\"ë\", \"e\"),\n",
        "    (\"í\", \"i\"),\n",
        "    (\"ï\", \"i\"),\n",
        "    (\"ö\", \"o\"),\n",
        "    (\"ü\", \"u\"),\n",
        "]\n",
        "\n",
        "\n",
        "def cleanup_text(inputs):\n",
        "    for src, dst in replacements:\n",
        "        inputs[\"normalized_text\"] = inputs[\"normalized_text\"].replace(src, dst)\n",
        "    return inputs\n",
        "\n",
        "\n",
        "dataset = dataset.map(cleanup_text)"
      ],
      "metadata": {
        "id": "5tWbbIh9bWs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Speakers"
      ],
      "metadata": {
        "id": "y0FaWzLgbcSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "speaker_counts = defaultdict(int)\n",
        "\n",
        "for speaker_id in dataset[\"speaker_id\"]:\n",
        "    speaker_counts[speaker_id] += 1"
      ],
      "metadata": {
        "id": "RaHNKX1fbdGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(speaker_counts.values(), bins=20)\n",
        "plt.ylabel(\"Speakers\")\n",
        "plt.xlabel(\"Examples\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1ZNXGCOmbmjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_speaker(speaker_id):\n",
        "    return 100 <= speaker_counts[speaker_id] <= 400\n",
        "\n",
        "dataset = dataset.filter(select_speaker, input_columns=[\"speaker_id\"])"
      ],
      "metadata": {
        "id": "1z3Qb7YObwH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(dataset[\"speaker_id\"]))"
      ],
      "metadata": {
        "id": "rfNVFJDTbysV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "id": "LtvbkFUeb0cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speaker embeddings"
      ],
      "metadata": {
        "id": "zZ7XbrH7cYDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from speechbrain.pretrained import EncoderClassifier\n",
        "\n",
        "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "speaker_model = EncoderClassifier.from_hparams(\n",
        "    source=spk_model_name,\n",
        "    run_opts={\"device\": device},\n",
        "    savedir=os.path.join(\"/tmp\", spk_model_name),\n",
        ")\n",
        "\n",
        "\n",
        "def create_speaker_embedding(waveform):\n",
        "    with torch.no_grad():\n",
        "        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))\n",
        "        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n",
        "        speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()\n",
        "    return speaker_embeddings"
      ],
      "metadata": {
        "id": "NTi6El9ocass"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing the dataset"
      ],
      "metadata": {
        "id": "wdoCRUszb5XP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(example):\n",
        "    audio = example[\"audio\"]\n",
        "\n",
        "    example = processor(\n",
        "        text=example[\"normalized_text\"],\n",
        "        audio_target=audio[\"array\"],\n",
        "        sampling_rate=audio[\"sampling_rate\"],\n",
        "        return_attention_mask=False,\n",
        "    )\n",
        "\n",
        "    # strip off the batch dimension\n",
        "    example[\"labels\"] = example[\"labels\"][0]\n",
        "\n",
        "    # use SpeechBrain to obtain x-vector\n",
        "    example[\"speaker_embeddings\"] = create_speaker_embedding(audio[\"array\"])\n",
        "\n",
        "    return example"
      ],
      "metadata": {
        "id": "27QYO5hYb6IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_example = prepare_dataset(dataset[0])\n",
        "list(processed_example.keys())"
      ],
      "metadata": {
        "id": "B30Ol33Xb93n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_example[\"speaker_embeddings\"].shape"
      ],
      "metadata": {
        "id": "yDdHSyuzcl7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(processed_example[\"labels\"].T)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "65EHr8T9c3e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)"
      ],
      "metadata": {
        "id": "49weWWkYc7RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_not_too_long(input_ids):\n",
        "    input_length = len(input_ids)\n",
        "    return input_length < 200\n",
        "\n",
        "\n",
        "dataset = dataset.filter(is_not_too_long, input_columns=[\"input_ids\"])\n",
        "len(dataset)"
      ],
      "metadata": {
        "id": "17Wnq7b8c96x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.train_test_split(test_size=0.1)"
      ],
      "metadata": {
        "id": "9shu9nOHdAMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data collator"
      ],
      "metadata": {
        "id": "OTfW80BbdCLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TTSDataCollatorWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(\n",
        "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n",
        "        label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n",
        "        speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n",
        "\n",
        "        # collate the inputs and targets into a batch\n",
        "        batch = processor.pad(\n",
        "            input_ids=input_ids, labels=label_features, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        batch[\"labels\"] = batch[\"labels\"].masked_fill(\n",
        "            batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100\n",
        "        )\n",
        "\n",
        "        # not used during fine-tuning\n",
        "        del batch[\"decoder_attention_mask\"]\n",
        "\n",
        "        # round down target lengths to multiple of reduction factor\n",
        "        if model.config.reduction_factor > 1:\n",
        "            target_lengths = torch.tensor(\n",
        "                [len(feature[\"input_values\"]) for feature in label_features]\n",
        "            )\n",
        "            target_lengths = target_lengths.new(\n",
        "                [\n",
        "                    length - length % model.config.reduction_factor\n",
        "                    for length in target_lengths\n",
        "                ]\n",
        "            )\n",
        "            max_length = max(target_lengths)\n",
        "            batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n",
        "\n",
        "        # also add in the speaker embeddings\n",
        "        batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "p0-9Rb3-dDNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = TTSDataCollatorWithPadding(processor=processor)"
      ],
      "metadata": {
        "id": "djMWgTwTdL9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "6GD-l80udOo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SpeechT5ForTextToSpeech\n",
        "\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "fnctF7PUdMjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "# disable cache during training since it's incompatible with gradient checkpointing\n",
        "model.config.use_cache = False\n",
        "\n",
        "# set language and task for generation and re-enable cache\n",
        "model.generate = partial(model.generate, use_cache=True)"
      ],
      "metadata": {
        "id": "eqUMZjnqdTnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"speecht5_finetuned_voxpopuli_nl\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    max_steps=4000,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=2,\n",
        "    save_steps=1000,\n",
        "    eval_steps=1000,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    greater_is_better=False,\n",
        "    label_names=[\"labels\"],\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "9iXOjcI-dXlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=processor,\n",
        ")"
      ],
      "metadata": {
        "id": "jIQg39g3dgI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "iWoAGzVxdlmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "A0zjZ66ednG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Inference"
      ],
      "metadata": {
        "id": "OPJeNr_9dpRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SpeechT5ForTextToSpeech.from_pretrained(\n",
        "    \"Tirendaz/speecht5_finetuned_voxpopuli_nl\"\n",
        ")"
      ],
      "metadata": {
        "id": "NReL8_q1do9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = dataset[\"test\"][304]\n",
        "speaker_embeddings = torch.tensor(example[\"speaker_embeddings\"]).unsqueeze(0)"
      ],
      "metadata": {
        "id": "JhqlpOVQdv-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"hallo allemaal, ik praat nederlands. groetjes aan iedereen!\""
      ],
      "metadata": {
        "id": "xTvUM8OEdxmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SpeechT5HifiGan\n",
        "\n",
        "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
        "speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)"
      ],
      "metadata": {
        "id": "x2PiKAtgd0Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "Audio(speech.numpy(), rate=16000)"
      ],
      "metadata": {
        "id": "8iqQ2GBQd10u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}